"""Implementaci√≥n de OCR usando Google Cloud Vision API - √ìptimo para escritura manual."""
import re
import os
from PIL import Image
from typing import List, Dict
import io

try:
    from google.cloud import vision
    GOOGLE_VISION_AVAILABLE = True
except ImportError:
    GOOGLE_VISION_AVAILABLE = False
    print("ADVERTENCIA: Google Cloud Vision no est√° instalado. Instalar con: pip install google-cloud-vision")

from ...domain.entities import CedulaRecord, RowData
from ...domain.ports import OCRPort, ConfigPort
from ..image import ImagePreprocessor


class GoogleVisionAdapter(OCRPort):
    """
    Implementaci√≥n de OCR usando Google Cloud Vision API con preprocesamiento avanzado.

    Google Cloud Vision es:
    - Estado del arte para escritura manual
    - Extremadamente preciso con n√∫meros manuscritos
    - 1,000 detecciones gratis al mes
    - Despu√©s: $1.50 por cada 1,000 detecciones

    Para 15 c√©dulas por imagen:
    - 1,000 im√°genes gratis = 15,000 c√©dulas gratis/mes

    MEJORAS DE PREPROCESAMIENTO:
    - Upscaling 3x para mejor resoluci√≥n (distingue 1 vs 7)
    - Reducci√≥n de ruido con fastNlMeansDenoising
    - Aumento de contraste adaptativo (CLAHE)
    - Sharpening para nitidez
    - Binarizaci√≥n m√©todo Otsu
    - Operaciones morfol√≥gicas para limpieza

    Attributes:
        config: Servicio de configuraci√≥n
        client: Cliente de Google Cloud Vision
        preprocessor: Pipeline de preprocesamiento de im√°genes
    """

    def __init__(self, config: ConfigPort):
        """
        Inicializa el servicio de OCR con Google Cloud Vision.

        Args:
            config: Servicio de configuraci√≥n
        """
        if not GOOGLE_VISION_AVAILABLE:
            raise ImportError("Google Cloud Vision no est√° instalado. Instalar con: pip install google-cloud-vision")

        self.config = config
        self.client = None
        self.last_raw_response = None  # Para guardar respuesta completa y extraer confianza por d√≠gito

        # Inicializar preprocesador con configuraci√≥n
        preprocessing_config = self.config.get('image_preprocessing', {})
        self.preprocessor = ImagePreprocessor(preprocessing_config)

        self._initialize_ocr()

    def _initialize_ocr(self) -> None:
        """Inicializa Google Cloud Vision API."""
        print("DEBUG Google Vision: Inicializando cliente...")

        try:
            # Intentar crear cliente con Application Default Credentials (ADC)
            # ADC busca credenciales en este orden:
            # 1. GOOGLE_APPLICATION_CREDENTIALS (archivo JSON)
            # 2. gcloud auth application-default login (credenciales de usuario)
            # 3. Compute Engine/App Engine/Cloud Run (credenciales de servicio)

            self.client = vision.ImageAnnotatorClient()

            print("‚úì Google Cloud Vision inicializado correctamente")
            print("‚úì Usando Application Default Credentials (ADC)")
            print("‚úì Modelo optimizado para escritura manual")

        except Exception as e:
            print(f"ERROR Google Vision: No se pudo inicializar: {e}")
            print("\nüí° Soluciones:")
            print("   1. Ejecutar: gcloud auth application-default login")
            print("   2. O configurar: GOOGLE_APPLICATION_CREDENTIALS con ruta a JSON")
            print("   3. Aseg√∫rate de habilitar Cloud Vision API en tu proyecto")
            raise

    def preprocess_image(self, image: Image.Image) -> Image.Image:
        """
        Preprocesa una imagen para Google Vision usando pipeline robusto.

        Aplica preprocesamiento intensivo para maximizar precisi√≥n sin aumentar costos:
        1. Upscaling (3x) - CR√çTICO para distinguir 1 vs 7
        2. Conversi√≥n a escala de grises
        3. Reducci√≥n de ruido (fastNlMeansDenoising)
        4. Aumento de contraste adaptativo (CLAHE)
        5. Sharpening para nitidez
        6. Binarizaci√≥n m√©todo Otsu
        7. Operaciones morfol√≥gicas (Close + Open)

        Args:
            image: Imagen PIL a preprocesar

        Returns:
            Imagen preprocesada y optimizada
        """
        print(f"\nDEBUG Google Vision: Imagen original {image.width}x{image.height}")

        # Verificar si el preprocesamiento est√° habilitado
        if not self.config.get('image_preprocessing.enabled', True):
            print("DEBUG Google Vision: Preprocesamiento deshabilitado, usando imagen original")
            if image.mode != 'RGB':
                image = image.convert('RGB')
            return image

        # Aplicar pipeline completo de preprocesamiento
        processed_image = self.preprocessor.preprocess(image)

        # Google Vision requiere RGB, convertir de vuelta si es necesario
        if processed_image.mode != 'RGB':
            processed_image = processed_image.convert('RGB')

        print(f"DEBUG Google Vision: Imagen procesada {processed_image.width}x{processed_image.height}")

        return processed_image

    def extract_cedulas(self, image: Image.Image) -> List[CedulaRecord]:
        """
        Extrae n√∫meros de c√©dula de una imagen usando Google Cloud Vision.

        Estrategia OPTIMIZADA:
        1. Enviar la imagen COMPLETA a Google Vision (1 SOLA llamada API)
        2. Google Vision detecta autom√°ticamente todas las l√≠neas de texto
        3. Extraer solo n√∫meros de 6-10 d√≠gitos que sean c√©dulas v√°lidas

        Esto consume solo 1 petici√≥n API en lugar de 15.

        Args:
            image: Imagen PIL a procesar

        Returns:
            Lista de registros de c√©dulas extra√≠das
        """
        if self.client is None:
            print("ERROR: Google Cloud Vision no est√° inicializado")
            return []

        print("DEBUG Google Vision: Iniciando extracci√≥n...")
        print("DEBUG Google Vision: Enviando imagen completa a API (1 sola llamada)")

        try:
            # Convertir imagen PIL a bytes
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format='PNG')
            img_byte_arr = img_byte_arr.getvalue()

            # Crear objeto Image de Google Vision
            vision_image = vision.Image(content=img_byte_arr)

            # OPTIMIZACI√ìN: Language hints para mejorar precisi√≥n en espa√±ol
            image_context = vision.ImageContext(language_hints=['es'])

            # Llamar a la API - DOCUMENT_TEXT_DETECTION detecta texto l√≠nea por l√≠nea
            # Esta es la √öNICA llamada API que hacemos
            print("DEBUG Google Vision: Llamando a DOCUMENT_TEXT_DETECTION (es)...")
            response = self.client.document_text_detection(
                image=vision_image,
                image_context=image_context
            )

            # Guardar respuesta completa para an√°lisis de confianza por d√≠gito
            self.last_raw_response = response

            if response.error.message:
                print(f"ERROR Google Vision API: {response.error.message}")
                return []

            print("‚úì Google Vision: Respuesta recibida (1 llamada API)")

            # Procesar respuesta - Google Vision detecta texto organizado por bloques/l√≠neas
            records = []

            # Opci√≥n 1: Usar full_text_annotation para obtener todo el texto
            if response.full_text_annotation:
                full_text = response.full_text_annotation.text
                print(f"DEBUG Google Vision: Texto completo detectado:\n{full_text}")

                # Procesar l√≠nea por l√≠nea
                lines = full_text.split('\n')
                print(f"DEBUG Google Vision: Detectadas {len(lines)} l√≠neas de texto")

                for idx, line in enumerate(lines):
                    if not line.strip():
                        continue

                    print(f"DEBUG Google Vision: L√≠nea {idx+1}: '{line.strip()}'")

                    # Extraer n√∫meros del texto
                    numbers = self._extract_numbers_from_text(line)

                    for num in numbers:
                        # Validar longitud de c√©dula (3-11 d√≠gitos)
                        if 3 <= len(num) <= 11:
                            # Usar factory method para crear con Value Objects
                            record = CedulaRecord.from_primitives(
                                cedula=num,
                                confidence=95.0  # Google Vision es muy confiable
                            )
                            records.append(record)
                            print(f"‚úì C√©dula extra√≠da: '{num}' ({len(num)} d√≠gitos)")
                        elif len(num) < 3:
                            print(f"‚úó Descartada (muy corta): '{num}' ({len(num)} d√≠gitos)")
                        else:
                            print(f"‚úó Descartada (muy larga): '{num}' ({len(num)} d√≠gitos)")

            print(f"‚úì Google Vision: Total llamadas API: 1 (√≥ptimo)")

            # Eliminar duplicados
            unique_records = self._remove_duplicates(records)

            print(f"DEBUG Google Vision: Total c√©dulas √∫nicas: {len(unique_records)}")

            return unique_records

        except Exception as e:
            print(f"ERROR Google Vision: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _split_image_into_lines(self, image: Image.Image) -> List[Image.Image]:
        """
        Divide la imagen en l√≠neas individuales (una por c√©dula).

        Usa divisi√≥n uniforme: imagen de 490px / 15 l√≠neas = ~32px por l√≠nea

        Args:
            image: Imagen completa

        Returns:
            Lista de sub-im√°genes (una por l√≠nea)
        """
        height = image.height
        width = image.width

        # Dividir en 15 l√≠neas uniformes
        expected_lines = 15
        line_height = height // expected_lines

        print(f"DEBUG Google Vision: Dividiendo imagen en {expected_lines} l√≠neas de {line_height}px cada una")

        regions = []

        for i in range(expected_lines):
            y1 = i * line_height
            y2 = min((i + 1) * line_height, height)

            if y2 - y1 > 10:  # Altura m√≠nima
                region = image.crop((0, y1, width, y2))
                regions.append(region)

        return regions

    def _extract_numbers_from_text(self, text: str) -> List[str]:
        """
        Extrae n√∫meros del texto reconocido.

        ESTRATEGIA MEJORADA:
        Cuando hay letras entre d√≠gitos (ej: "107 116C1931"), NO separar
        en m√∫ltiples n√∫meros. En su lugar, eliminar TODAS las letras y
        espacios, dejando solo d√≠gitos continuos.

        Esto evita que una c√©dula de 10 d√≠gitos se divida en m√∫ltiples fragmentos.

        Args:
            text: Texto reconocido por Google Vision (ej: "107 116C1931")

        Returns:
            Lista con UN string num√©rico por l√≠nea (ej: ["1071161931"])
        """
        # Eliminar TODOS los caracteres que no sean d√≠gitos
        # Esto incluye: letras, espacios, puntos, comas, guiones, etc.
        text_clean = re.sub(r'[^\d]', '', text)

        # Si queda alg√∫n n√∫mero, retornarlo como una sola c√©dula
        if text_clean:
            return [text_clean]
        else:
            return []

    def _remove_duplicates(self, records: List[CedulaRecord]) -> List[CedulaRecord]:
        """
        Elimina registros duplicados, manteniendo el de mayor confianza.

        Args:
            records: Lista de registros

        Returns:
            Lista sin duplicados
        """
        seen = {}

        for record in records:
            # Usar .value ya que cedula es ahora CedulaNumber (Value Object)
            cedula_key = record.cedula.value
            # Comparar confidence usando .as_percentage() ya que es ConfidenceScore
            if cedula_key not in seen or record.confidence.as_percentage() > seen[cedula_key].confidence.as_percentage():
                seen[cedula_key] = record

        return list(seen.values())

    def get_character_confidences(self, text: str) -> Dict[str, any]:
        """
        Extrae la confianza individual de cada car√°cter en el texto detectado.

        Google Vision API retorna confianza a nivel de s√≠mbolo/car√°cter en:
        - full_text_annotation.pages[].blocks[].paragraphs[].words[].symbols[]

        Args:
            text: El texto (c√©dula) para el cual queremos las confianzas

        Returns:
            Dict con:
            - 'confidences': List[float] con confianza de cada car√°cter (0.0-1.0)
            - 'positions': List[int] con posici√≥n de cada car√°cter
            - 'average': float con confianza promedio
            - 'source': str identificando el origen

        Example:
            >>> confidences = adapter.get_character_confidences("1036221525")
            >>> confidences
            {
                'confidences': [0.98, 0.95, 0.97, 0.94, ...],
                'positions': [0, 1, 2, 3, ...],
                'average': 0.956,
                'source': 'google_vision'
            }

        Raises:
            ValueError: Si no hay respuesta disponible (ejecuta extract_cedulas() primero)
        """
        if not self.last_raw_response:
            raise ValueError("No hay respuesta disponible. Ejecuta extract_cedulas() primero.")

        if not self.last_raw_response.full_text_annotation:
            print("ADVERTENCIA: No hay full_text_annotation en respuesta de Google Vision")
            # Fallback: confianza uniforme
            return {
                'confidences': [0.85] * len(text),
                'positions': list(range(len(text))),
                'average': 0.85,
                'source': 'google_vision'
            }

        # Limpiar el texto buscado (eliminar espacios, puntos, etc)
        text_clean = text.replace(' ', '').replace('.', '').replace(',', '').replace('-', '')

        # Extraer todos los s√≠mbolos con sus confianzas
        all_symbols = []

        # Iterar sobre la estructura jer√°rquica de Google Vision
        for page in self.last_raw_response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        # Construir el texto de la palabra
                        word_text = ''.join([symbol.text for symbol in word.symbols])

                        # Obtener confianza de la palabra (si existe)
                        # Google Vision da confidence a nivel de palabra, no de s√≠mbolo individual
                        word_confidence = word.confidence if hasattr(word, 'confidence') else 0.95

                        # Agregar cada s√≠mbolo con la confianza de su palabra
                        for symbol in word.symbols:
                            symbol_conf = symbol.confidence if hasattr(symbol, 'confidence') else word_confidence
                            all_symbols.append({
                                'text': symbol.text,
                                'confidence': symbol_conf
                            })

        # Buscar el texto en los s√≠mbolos detectados
        # Construir string de todos los s√≠mbolos
        all_text = ''.join([s['text'] for s in all_symbols])
        all_text_clean = ''.join([c for c in all_text if c.isdigit()])

        # Intentar encontrar el texto buscado en el texto detectado
        confidences = []
        positions = []

        if text_clean in all_text_clean:
            # Encontrado - extraer confianzas correspondientes
            start_idx = all_text_clean.index(text_clean)

            # Mapear √≠ndices de all_text_clean a all_symbols
            digit_counter = 0
            symbol_idx = 0

            for symbol in all_symbols:
                if symbol['text'].isdigit():
                    if digit_counter >= start_idx and digit_counter < start_idx + len(text_clean):
                        confidences.append(symbol['confidence'])
                        positions.append(digit_counter - start_idx)
                    digit_counter += 1
        else:
            # No encontrado - usar confianza uniforme basada en promedio general
            print(f"ADVERTENCIA: Texto '{text_clean}' no encontrado en respuesta de Google Vision")
            print(f"DEBUG: Texto detectado: '{all_text_clean[:100]}...'")

            # Calcular confianza promedio de todos los s√≠mbolos num√©ricos
            numeric_symbols = [s for s in all_symbols if s['text'].isdigit()]
            avg_conf = sum(s['confidence'] for s in numeric_symbols) / len(numeric_symbols) if numeric_symbols else 0.90

            confidences = [avg_conf] * len(text_clean)
            positions = list(range(len(text_clean)))

        # Calcular promedio
        average = sum(confidences) / len(confidences) if confidences else 0.0

        return {
            'confidences': confidences,
            'positions': positions,
            'average': average,
            'source': 'google_vision'
        }

    def extract_full_form_data(self, image: Image.Image, expected_rows: int = 15) -> List[RowData]:
        """
        Extrae datos completos del formulario manuscrito (nombres + c√©dulas).

        ‚ö° OPTIMIZADO - UNA SOLA LLAMADA API (antes: 15 llamadas)

        ESTRATEGIA OPTIMIZADA:
        1. Enviar imagen COMPLETA a Google Vision (1 llamada API)
        2. Google Vision detecta TODO el texto con coordenadas (bounding boxes)
        3. Organizar texto en renglones basado en coordenada Y
        4. Separar nombres/c√©dulas por columna basado en coordenada X
        5. Detectar renglones vac√≠os

        Esto reduce de 15 llamadas API a 1 SOLA llamada.
        Mejora: 93% reducci√≥n de llamadas API
        Costo: 93% menor
        Velocidad: ~10x m√°s r√°pido

        Args:
            image: Imagen PIL del formulario manuscrito completo
            expected_rows: N√∫mero esperado de renglones (default: 15)

        Returns:
            Lista de RowData (uno por rengl√≥n)
        """
        if self.client is None:
            print("ERROR: Google Cloud Vision no est√° inicializado")
            return []

        print(f"\n{'='*70}")
        print("‚ö° EXTRACCI√ìN OPTIMIZADA - Nombres + C√©dulas (1 SOLA LLAMADA API)")
        print(f"{'='*70}")
        print(f"Imagen: {image.width}x{image.height} px")
        print(f"Renglones esperados: {expected_rows}")

        try:
            # ========== PASO 1: UNA SOLA LLAMADA API ==========
            print("\n[PASO 1] Enviando imagen completa a Google Vision API...")

            # Convertir imagen PIL a bytes
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format='PNG')
            img_byte_arr = img_byte_arr.getvalue()

            # Crear objeto Image de Google Vision
            vision_image = vision.Image(content=img_byte_arr)

            # OPTIMIZACI√ìN: Language hints para mejorar precisi√≥n en espa√±ol
            image_context = vision.ImageContext(language_hints=['es'])

            # ‚ö° √öNICA LLAMADA API - DOCUMENT_TEXT_DETECTION
            response = self.client.document_text_detection(
                image=vision_image,
                image_context=image_context
            )

            if response.error.message:
                print(f"‚úó Error API: {response.error.message}")
                return [self._create_empty_row(i) for i in range(expected_rows)]

            print("‚úì Respuesta recibida (1 llamada API - √ìPTIMO)")

            # Verificar si hay texto detectado
            if not response.full_text_annotation or not response.full_text_annotation.text.strip():
                print("‚úó No se detect√≥ texto en la imagen")
                return [self._create_empty_row(i) for i in range(expected_rows)]

            # ========== PASO 2: ORGANIZAR TEXTO POR RENGLONES ==========
            print(f"\n[PASO 2] Organizando texto en {expected_rows} renglones...")

            # Extraer todos los bloques de texto con coordenadas
            all_blocks = self._extract_text_blocks_with_coords(response)
            print(f"‚úì Detectados {len(all_blocks)} bloques de texto")

            # Dividir bloques en renglones basados en coordenada Y
            rows_blocks = self._assign_blocks_to_rows(
                all_blocks,
                image.height,
                expected_rows
            )

            # ========== PASO 3: PROCESAR CADA RENGL√ìN ==========
            print(f"\n[PASO 3] Procesando {expected_rows} renglones...")

            all_rows_data = []

            for row_idx in range(expected_rows):
                blocks_in_row = rows_blocks.get(row_idx, [])

                if not blocks_in_row:
                    # Rengl√≥n vac√≠o - no hay bloques de texto
                    row_data = self._create_empty_row(row_idx)
                    all_rows_data.append(row_data)
                    print(f"  [{row_idx + 1:2d}] ‚Üí [VAC√çO]")
                else:
                    # Procesar bloques del rengl√≥n
                    row_data = self._process_row_blocks(
                        blocks_in_row,
                        row_idx,
                        image.width
                    )
                    all_rows_data.append(row_data)

                    # Log resultado
                    if row_data.is_empty:
                        print(f"  [{row_idx + 1:2d}] ‚Üí [VAC√çO] (confianza baja)")
                    else:
                        print(f"  [{row_idx + 1:2d}] ‚Üí Nombres: '{row_data.nombres_manuscritos}' | C√©dula: '{row_data.cedula}'")

            # ========== RESUMEN ==========
            print(f"\n{'='*70}")
            print(f"RESUMEN: {len(all_rows_data)} renglones procesados")
            vacios = sum(1 for r in all_rows_data if r.is_empty)
            con_datos = len(all_rows_data) - vacios
            print(f"  - Con datos: {con_datos}")
            print(f"  - Vac√≠os: {vacios}")
            print(f"  - ‚ö° Total llamadas API: 1 (93% reducci√≥n vs antes)")
            print(f"{'='*70}\n")

            return all_rows_data

        except Exception as e:
            print(f"‚úó Error en extracci√≥n: {e}")
            import traceback
            traceback.print_exc()
            return [self._create_empty_row(i) for i in range(expected_rows)]

    def _extract_text_blocks_with_coords(self, response) -> List[Dict]:
        """
        Extrae todos los bloques de texto con sus coordenadas de bounding box.

        Args:
            response: Respuesta de Google Vision API

        Returns:
            Lista de diccionarios con:
                - text: Texto del bloque
                - x: Coordenada X promedio (centro horizontal)
                - y: Coordenada Y promedio (centro vertical)
                - confidence: Confianza del OCR
                - vertices: V√©rtices del bounding box
        """
        blocks = []

        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                # Calcular coordenadas promedio del bloque
                vertices = block.bounding_box.vertices
                if not vertices:
                    continue

                avg_x = sum(v.x for v in vertices) / len(vertices)
                avg_y = sum(v.y for v in vertices) / len(vertices)

                # Extraer texto del bloque
                block_text = ""
                block_confidence = 0.0
                word_count = 0

                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        word_text = ''.join([symbol.text for symbol in word.symbols])
                        block_text += word_text + " "
                        block_confidence += word.confidence
                        word_count += 1

                block_text = block_text.strip()

                if word_count > 0:
                    block_confidence /= word_count

                if block_text:  # Solo agregar bloques con texto
                    blocks.append({
                        'text': block_text,
                        'x': avg_x,
                        'y': avg_y,
                        'confidence': block_confidence,
                        'vertices': vertices
                    })

        return blocks

    def _assign_blocks_to_rows(
        self,
        blocks: List[Dict],
        image_height: int,
        num_rows: int
    ) -> Dict[int, List[Dict]]:
        """
        Asigna bloques de texto a renglones bas√°ndose en coordenada Y.

        Divide la imagen en renglones uniformes y asigna cada bloque
        al rengl√≥n m√°s cercano seg√∫n su coordenada Y.

        Args:
            blocks: Lista de bloques con coordenadas
            image_height: Altura de la imagen en p√≠xeles
            num_rows: N√∫mero de renglones esperados

        Returns:
            Diccionario {row_index: [bloques]}
        """
        row_height = image_height / num_rows
        rows_blocks = {i: [] for i in range(num_rows)}

        for block in blocks:
            # Determinar a qu√© rengl√≥n pertenece seg√∫n coordenada Y
            row_idx = int(block['y'] / row_height)

            # Asegurar que est√° dentro del rango
            row_idx = max(0, min(row_idx, num_rows - 1))

            rows_blocks[row_idx].append(block)

        return rows_blocks

    def _process_row_blocks(
        self,
        blocks: List[Dict],
        row_index: int,
        image_width: int
    ) -> RowData:
        """
        Procesa bloques de un rengl√≥n separando nombres y c√©dula.

        Separa los bloques en dos columnas bas√°ndose en coordenada X:
        - Columna izquierda (0-60% del ancho): NOMBRES
        - Columna derecha (60-100% del ancho): C√âDULA

        Args:
            blocks: Bloques de texto del rengl√≥n
            row_index: √çndice del rengl√≥n
            image_width: Ancho de la imagen

        Returns:
            RowData con nombres, c√©dula y confianza
        """
        # L√≠mite de columnas (60% del ancho)
        column_boundary = image_width * 0.6

        nombres_parts = []
        cedula_parts = []
        nombres_confidences = []
        cedula_confidences = []

        # Clasificar bloques por columna
        for block in blocks:
            if block['x'] < column_boundary:
                # Columna izquierda - NOMBRES
                nombres_parts.append(block['text'])
                nombres_confidences.append(block['confidence'])
            else:
                # Columna derecha - C√âDULA
                cedula_parts.append(block['text'])
                cedula_confidences.append(block['confidence'])

        # Combinar partes
        nombres = ' '.join(nombres_parts).strip()
        cedula_raw = ' '.join(cedula_parts).strip()

        # OPTIMIZACI√ìN: Corregir errores comunes de OCR antes de limpiar
        cedula = self._corregir_errores_ocr_cedula(cedula_raw)

        # Calcular confianza promedio
        nombres_conf = sum(nombres_confidences) / len(nombres_confidences) if nombres_confidences else 0.0
        cedula_conf = sum(cedula_confidences) / len(cedula_confidences) if cedula_confidences else 0.0

        confidence = {
            'nombres': nombres_conf,
            'cedula': cedula_conf
        }

        # Crear texto raw para debugging
        raw_text = f"{nombres} | {cedula_raw}".strip()

        # Detectar si es rengl√≥n vac√≠o basado en umbral de confianza
        min_confidence = self.config.get('ocr.google_vision.confidence_threshold', 0.30)
        is_empty = (
            (not nombres and not cedula) or
            (confidence.get('nombres', 0) < min_confidence and confidence.get('cedula', 0) < min_confidence) or
            (len(nombres) < 2 and len(cedula) < 6)  # Muy poco texto
        )

        # Usar factory method para crear RowData con Value Objects
        return RowData.from_primitives(
            row_index=row_index,
            nombres_manuscritos=nombres,
            cedula=cedula,
            is_empty=is_empty,
            confidence=confidence,
            raw_text=raw_text
        )

    # ========== M√âTODOS LEGACY (DEPRECADOS) ==========
    # Los siguientes m√©todos ya NO se usan en extract_full_form_data optimizado.
    # Se mantienen por compatibilidad pero no se llaman.

    def _split_image_into_rows(self, image: Image.Image, num_rows: int) -> List[Image.Image]:
        """
        [DEPRECADO] Divide la imagen en renglones horizontales uniformes.

        Este m√©todo ya NO se usa en la versi√≥n optimizada de extract_full_form_data.
        La versi√≥n optimizada procesa la imagen completa en 1 sola llamada API.

        Args:
            image: Imagen completa del formulario
            num_rows: N√∫mero de renglones a crear

        Returns:
            Lista de sub-im√°genes (una por rengl√≥n)
        """
        height = image.height
        width = image.width
        row_height = height // num_rows

        print(f"Divisi√≥n: {height}px / {num_rows} renglones = {row_height}px por rengl√≥n")

        rows = []
        for i in range(num_rows):
            y1 = i * row_height
            y2 = min((i + 1) * row_height, height)

            if y2 - y1 > 5:  # Altura m√≠nima
                row = image.crop((0, y1, width, y2))
                rows.append(row)

        return rows

    def _process_single_row(self, row_image: Image.Image, row_index: int) -> RowData:
        """
        Procesa un rengl√≥n individual extrayendo nombres y c√©dula.

        Estrategia de separaci√≥n por columnas:
        - Columna izquierda (0-50% del ancho): NOMBRES
        - Columna centro (50-100% del ancho): C√âDULA

        Args:
            row_image: Imagen del rengl√≥n a procesar
            row_index: √çndice del rengl√≥n (0-14)

        Returns:
            RowData con nombres, c√©dula, y estado de vac√≠o
        """
        try:
            # Convertir imagen PIL a bytes
            img_byte_arr = io.BytesIO()
            row_image.save(img_byte_arr, format='PNG')
            img_byte_arr = img_byte_arr.getvalue()

            # Crear objeto Image de Google Vision
            vision_image = vision.Image(content=img_byte_arr)

            # OPTIMIZACI√ìN: Language hints para mejorar precisi√≥n en espa√±ol
            image_context = vision.ImageContext(language_hints=['es'])

            # Llamar a la API - DOCUMENT_TEXT_DETECTION
            response = self.client.document_text_detection(
                image=vision_image,
                image_context=image_context
            )

            if response.error.message:
                print(f"  ‚úó Error API: {response.error.message}")
                return self._create_empty_row(row_index)

            # Verificar si hay texto detectado
            if not response.full_text_annotation or not response.full_text_annotation.text.strip():
                print(f"  ‚Üí Sin texto detectado (rengl√≥n vac√≠o)")
                return self._create_empty_row(row_index)

            # Extraer texto completo para debugging
            full_text = response.full_text_annotation.text.strip()
            print(f"  ‚Üí Texto detectado: '{full_text}'")

            # Separar nombres y c√©dula usando coordenadas de boundingBox
            nombres, cedula, confidence = self._separate_nombres_cedula(
                response,
                row_image.width
            )

            # Detectar si es rengl√≥n vac√≠o basado en umbral de confianza
            min_confidence = self.config.get('ocr.google_vision.confidence_threshold', 0.30)
            is_empty = (
                (not nombres and not cedula) or
                (confidence.get('nombres', 0) < min_confidence and confidence.get('cedula', 0) < min_confidence) or
                (len(nombres) < 2 and len(cedula) < 6)  # Muy poco texto
            )

            # Usar factory method para crear RowData con Value Objects
            return RowData.from_primitives(
                row_index=row_index,
                nombres_manuscritos=nombres,
                cedula=cedula,
                is_empty=is_empty,
                confidence=confidence,
                raw_text=full_text
            )

        except Exception as e:
            print(f"  ‚úó Error procesando rengl√≥n: {e}")
            return self._create_empty_row(row_index)

    def _separate_nombres_cedula(
        self,
        response,
        image_width: int
    ) -> tuple[str, str, Dict[str, float]]:
        """
        Separa nombres y c√©dula bas√°ndose en posici√≥n horizontal del texto.

        Columnas del formulario manuscrito:
        - Izquierda (0-60% del ancho): NOMBRES
        - Centro (60-100% del ancho): C√âDULA

        Args:
            response: Respuesta de Google Vision API
            image_width: Ancho de la imagen del rengl√≥n

        Returns:
            (nombres, cedula, confidence_dict)
        """
        # L√≠mite de columnas (60% del ancho)
        column_boundary = image_width * 0.6

        nombres_parts = []
        cedula_parts = []
        nombres_confidences = []
        cedula_confidences = []

        # Iterar sobre los bloques/palabras detectadas
        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    # Obtener bounding box del p√°rrafo
                    vertices = paragraph.bounding_box.vertices
                    if not vertices:
                        continue

                    # Calcular posici√≥n X promedio
                    avg_x = sum(v.x for v in vertices) / len(vertices)

                    # Extraer texto del p√°rrafo
                    paragraph_text = ""
                    paragraph_confidence = 0.0
                    word_count = 0

                    for word in paragraph.words:
                        word_text = ''.join([symbol.text for symbol in word.symbols])
                        paragraph_text += word_text + " "
                        paragraph_confidence += word.confidence
                        word_count += 1

                    paragraph_text = paragraph_text.strip()
                    if word_count > 0:
                        paragraph_confidence /= word_count

                    # Clasificar en columna izquierda (nombres) o centro (c√©dula)
                    if avg_x < column_boundary:
                        # Columna izquierda - NOMBRES
                        nombres_parts.append(paragraph_text)
                        nombres_confidences.append(paragraph_confidence)
                    else:
                        # Columna centro - C√âDULA
                        cedula_parts.append(paragraph_text)
                        cedula_confidences.append(paragraph_confidence)

        # Combinar partes
        nombres = ' '.join(nombres_parts).strip()
        cedula_raw = ' '.join(cedula_parts).strip()

        # OPTIMIZACI√ìN: Corregir errores comunes de OCR antes de limpiar
        cedula = self._corregir_errores_ocr_cedula(cedula_raw)

        # Calcular confianza promedio
        nombres_conf = sum(nombres_confidences) / len(nombres_confidences) if nombres_confidences else 0.0
        cedula_conf = sum(cedula_confidences) / len(cedula_confidences) if cedula_confidences else 0.0

        confidence = {
            'nombres': nombres_conf,
            'cedula': cedula_conf
        }

        return nombres, cedula, confidence

    def _corregir_errores_ocr_cedula(self, cedula: str) -> str:
        """
        Corrige errores comunes de OCR en c√©dulas manuscritas.

        OPTIMIZACI√ìN CR√çTICA del prompt.txt:
        Aplica matriz de confusi√≥n para errores t√≠picos en escritura manual.

        Correcciones implementadas:
        - l, I, | ‚Üí 1 (confusi√≥n con n√∫mero 1)
        - O, o ‚Üí 0 (confusi√≥n con cero)
        - S, s ‚Üí 5 (confusi√≥n con 5)
        - B ‚Üí 8 (confusi√≥n con 8)
        - Z, z ‚Üí 2 (confusi√≥n con 2)
        - G ‚Üí 6 (confusi√≥n con 6)

        Args:
            cedula: String de c√©dula potencialmente con errores

        Returns:
            C√©dula corregida con solo d√≠gitos num√©ricos

        Example:
            "lO23456" ‚Üí "1023456"
            "B765432I" ‚Üí "87654321"
        """
        if not cedula:
            return cedula

        # Matriz de correcci√≥n de errores comunes
        COMMON_ERRORS = {
            'l': '1', 'I': '1', '|': '1',  # Confusi√≥n con 1
            'O': '0', 'o': '0',             # Confusi√≥n con 0
            'S': '5', 's': '5',             # Confusi√≥n con 5
            'B': '8',                        # Confusi√≥n con 8
            'Z': '2', 'z': '2',             # Confusi√≥n con 2
            'G': '6',                        # Confusi√≥n con 6
        }

        # Aplicar correcciones car√°cter por car√°cter
        cedula_corregida = ""
        correcciones_aplicadas = []

        for char in cedula:
            if char in COMMON_ERRORS:
                char_corregido = COMMON_ERRORS[char]
                cedula_corregida += char_corregido
                correcciones_aplicadas.append(f"{char}‚Üí{char_corregido}")
            else:
                cedula_corregida += char

        # Log correcciones si se aplicaron
        if correcciones_aplicadas:
            print(f"  üîß Correcciones OCR aplicadas: {', '.join(correcciones_aplicadas)}")
            print(f"     Antes: '{cedula}' ‚Üí Despu√©s: '{cedula_corregida}'")

        # Filtrar solo d√≠gitos num√©ricos
        cedula_final = ''.join(filter(str.isdigit, cedula_corregida))

        return cedula_final

    def _create_empty_row(self, row_index: int) -> RowData:
        """
        Crea un RowData vac√≠o para renglones sin datos.

        Args:
            row_index: √çndice del rengl√≥n

        Returns:
            RowData marcado como vac√≠o
        """
        # Usar factory method para crear RowData con Value Objects
        return RowData.from_primitives(
            row_index=row_index,
            nombres_manuscritos="",
            cedula="",
            is_empty=True,
            confidence={},
            raw_text=None
        )

    def _extract_text_blocks_with_positions(self, response) -> List[Dict]:
        """
        Extrae palabras individuales con coordenadas (NO bloques completos).

        CAMBIO IMPORTANTE: Ahora extrae a nivel de PALABRA en lugar de BLOCK.
        Esto permite que filter_nombres() agrupe correctamente nombres que est√°n
        separados espacialmente, evitando que Google Vision agrupe nombres diferentes
        en un solo bloque gigante.

        Args:
            response: Respuesta de Google Vision API

        Returns:
            Lista de palabras con {text, x, y, width, height, confidence}
        """
        word_blocks = []

        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        # Obtener vertices de esta palabra espec√≠fica
                        vertices = word.bounding_box.vertices
                        if not vertices or len(vertices) < 4:
                            continue

                        # Calcular bounding box de la palabra
                        min_x = min(v.x for v in vertices)
                        max_x = max(v.x for v in vertices)
                        min_y = min(v.y for v in vertices)
                        max_y = max(v.y for v in vertices)

                        # Extraer texto de la palabra
                        word_text = ''.join([symbol.text for symbol in word.symbols])

                        if word_text.strip():
                            word_blocks.append({
                                'text': word_text.strip(),
                                'x': min_x,
                                'y': min_y,
                                'width': max_x - min_x,
                                'height': max_y - min_y,
                                'confidence': word.confidence
                            })

        return word_blocks

    def extract_name_cedula_pairs(self, image: Image.Image) -> List[Dict]:
        """
        Extrae pares nombre-c√©dula usando post-procesamiento + proximidad espacial.

        Flujo:
        1. Extraer TODO el texto con coordenadas
        2. Post-procesamiento: filtrar nombres
        3. Post-procesamiento: filtrar c√©dulas
        4. Emparejar por proximidad espacial

        Returns:
            Lista de pares nombre-c√©dula correctamente emparejados
        """
        from .spatial_pairing import SpatialPairing

        print("\n" + "="*80)
        print("GOOGLE VISION: Extracci√≥n de pares nombre-c√©dula")
        print("="*80)

        # 1. Preprocesar imagen
        processed_image = self.preprocess_image(image)

        # 2. Extraer TODO el texto con coordenadas
        print("[1/4] Extrayendo texto con coordenadas...")
        img_byte_arr = io.BytesIO()
        processed_image.save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()

        vision_image = vision.Image(content=img_byte_arr)
        image_context = vision.ImageContext(language_hints=['es'])

        response = self.client.document_text_detection(
            image=vision_image,
            image_context=image_context
        )

        self.last_raw_response = response

        if response.error.message:
            raise Exception(f"Google Vision error: {response.error.message}")

        # Extraer bloques con posiciones
        all_blocks = self._extract_text_blocks_with_positions(response)
        print(f"  ‚úì Extra√≠dos {len(all_blocks)} bloques de texto")

        # 3. Post-procesamiento: filtrar nombres
        print("[2/4] Post-procesando nombres...")
        nombres = SpatialPairing.filter_nombres(all_blocks)
        print(f"  ‚úì Detectados {len(nombres)} nombres")
        for n in nombres:
            print(f"    - {n['text']} (conf: {n['confidence']:.2%})")

        # 4. Post-procesamiento: filtrar c√©dulas
        print("[3/4] Post-procesando c√©dulas...")
        cedulas = SpatialPairing.filter_cedulas(all_blocks)
        print(f"  ‚úì Detectadas {len(cedulas)} c√©dulas")
        for c in cedulas:
            print(f"    - {c['text']} (conf: {c['confidence']:.2%})")

        # 5. Emparejar por proximidad espacial
        print("[4/4] Emparejando por proximidad espacial...")
        pares = SpatialPairing.pair_by_proximity(nombres, cedulas, verbose=True)
        print(f"  ‚úì Emparejados {len(pares)} pares")

        print("="*80 + "\n")

        return pares

